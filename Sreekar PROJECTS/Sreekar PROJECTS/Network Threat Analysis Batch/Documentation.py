# Databricks notebook source
# MAGIC %md
# MAGIC # Network Threat Analysis - Batch 

# COMMAND ----------

# MAGIC %md
# MAGIC ### Description

# COMMAND ----------

# MAGIC %md
# MAGIC The Realtime Network Threat Analysis Dashboard is a sophisticated solution crafted to continuously monitor and analyze network events in real-time. 
# MAGIC Its primary goal is to swiftly detect potential security threats, enabling proactive measures to mitigate risks effectively. By processing vast amounts of network event data generated by various firewalls across the company, the dashboard provides actionable insights to bolster the organization's cybersecurity posture.

# COMMAND ----------

# MAGIC %md
# MAGIC ### Architecture

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC The architecture is designed around three main layers: Bronze, Silver, and Gold.
# MAGIC
# MAGIC Bronze Layer: This layer serves as the initial data ingestion point. It utilizes Autoloader to seamlessly load raw network event data from all firewalls into an Azure Data Lake Storage (ADLS) folder. Subsequently, a Bronze job processes this data, adhering to a predefined schema, and stores it efficiently in Delta tables with partitioning for optimized querying.
# MAGIC
# MAGIC Silver Layer: This layer is responsible for data validation, transformation and data enrichment. Data processing and refinement occur in this layer. It comprises two notebooks:
# MAGIC
# MAGIC Notebook 1 (Data Validations): Responsible for validating the integrity and quality of the incoming data. It identifies and separates valid and invalid records, flattens nested structures such as arrays (e.g., user_detail), and stores them in Delta tables. This ensures that only clean and accurate data is passed on to subsequent stages of analysis.
# MAGIC
# MAGIC Notebook 2 (Data Enrichment): Focuses on enhancing the raw data by enriching it through various operations, including joins with other relevant tables. This step enhances the context and depth of the data, facilitating more insightful analysis in subsequent layers.So, here we are joining the device info table, to get the enriched data. We are also removing duplicates by firewall name, from the device info table to make sure there are no duplicates in the network records.
# MAGIC
# MAGIC Additional Notebooks: Depending on the complexity of data processing and the need for specific transformations or analyses, additional notebooks can be introduced. For example, Notebook 3 might handle outlier detection and treatment, while Notebook 4 could perform anomaly detection algorithms.
# MAGIC
# MAGIC Gold Layer: This layer is dedicated to advanced analytics and KPI (Key Performance Indicator) calculation:
# MAGIC
# MAGIC Notebook 1 (KPI or Metric Aggregates Calculation): Performs complex calculations to derive meaningful KPIs and metric aggregates from the enriched data. These calculations typically involve aggregating threat information based on factors such as location, time (hour, week, month), and other relevant dimensions. The results are stored in separate tables for easy access and visualization.
# MAGIC
# MAGIC <img src ='/files/tables/Flow_Diagram_jpeg.jpg' width="1000" height="800">
# MAGIC
# MAGIC
# MAGIC <img src ='https://cms.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png'>

# COMMAND ----------

# MAGIC %md
# MAGIC ### Tech Stack ( Need to elaborate)
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC 1) Data Storage: Autoloader, Streaming ingestion, Azure Data Lake Storage (ADLS)
# MAGIC
# MAGIC 2) Data Processing: Apache Spark (utilized within notebooks) or Azure Databricks, Delta Lake, Data partitioning.
# MAGIC
# MAGIC 3) Querying: Delta tables, Join operations, Advanced analytics, SQL for data manipulation and extraction, KPIs, Metric aggregates
# MAGIC
# MAGIC 4) Visualization: Power BI, PowerBI integration, Customizable dashboard development 
# MAGIC
# MAGIC <img src ='https://editor.analyticsvidhya.com/uploads/327260_qeklvRG4woQtCNVT.png'>
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Team Structure ( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Project Manager:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Design and implement data pipelines.
# MAGIC Configure Autoloader for data ingestion.
# MAGIC Manage Delta tables.
# MAGIC Key Skills: Project management, data engineering, understanding of data architecture and storage solutions.
# MAGIC
# MAGIC ###Scrum Master:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Facilitate Agile ceremonies such as sprint planning, daily stand-ups, sprint reviews, and retrospectives.
# MAGIC Remove impediments and ensure the team stays focused on delivering tasks.
# MAGIC Key Skills: Agile methodologies, leadership, communication, problem-solving.
# MAGIC
# MAGIC ###Product Owner:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Define the product vision and prioritize the product backlog.
# MAGIC Act as the liaison between stakeholders and the development team.
# MAGIC Key Skills: Product management, stakeholder management, understanding of business requirements.
# MAGIC
# MAGIC ###Data Engineer:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Conduct data validations to ensure data quality.
# MAGIC Enrich raw data with additional context.
# MAGIC Perform advanced analytics on the data.
# MAGIC Key Skills: Data validation, data enrichment, analytics, programming (e.g., Python, SQL).
# MAGIC
# MAGIC ###Quality Assurance Engineer:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Develop test plans, test cases, and test scripts.
# MAGIC Execute testing activities to ensure the quality of deliverables.
# MAGIC Report and track defects and issues.
# MAGIC Key Skills: Testing methodologies, test automation, attention to detail.
# MAGIC
# MAGIC ###Senior Data Architect:
# MAGIC
# MAGIC ####Responsibilities:
# MAGIC Design and implement data architecture solutions.
# MAGIC Provide guidance on data modeling, storage, and processing.
# MAGIC Ensure scalability, reliability, and performance of data systems.
# MAGIC Key Skills: Data architecture, data modeling, database technologies, leadership.
# MAGIC
# MAGIC
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Task Breakdown( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC 1) Analyzing of Data:
# MAGIC   Analyzing of Data in logsfiles which are coming from the network devices and create three models
# MAGIC
# MAGIC     1) Conceptual Model: Conceptual model is adding the coloumns
# MAGIC     2) Logical Model: Logical Model is  defining the relationships between the tables and definining the primary keys and foreign keys.
# MAGIC     3) Physical Model:Physical Model is defining the partitioning and zordering thge data.
# MAGIC
# MAGIC 2) Bronze Layer:
# MAGIC   
# MAGIC     Task: Set up the Bronze layer for ingesting and storing raw data.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Use faker function to randomly generate the data for two tables.
# MAGIC     2) Configure Autoloader in Azure Data Lake Storage for automatic ingestion of data logs.
# MAGIC     3) Develop scripts or jobs in Databricks to process incoming data and store it in Delta tables in the Bronze layer.
# MAGIC     4) Implement partitioning strategies for efficient data storage and retrieval. 
# MAGIC
# MAGIC 3) Silver Layer:
# MAGIC     Task: Develop the Silver layer for data refinement and validation.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Create Notebook 1 for data validation, including checks for data integrity, consistency, and completeness.
# MAGIC     2) Implement transformations to clean and normalize the data.
# MAGIC     3) Store validated data in Delta tables within the Silver layer.
# MAGIC     4) Develop Notebook 2 for data enrichment, such as joining with reference tables or adding derived columns.
# MAGIC
# MAGIC 4) Gold Layer:
# MAGIC     Task: Design the Gold layer for advanced analytics and KPI calculation.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Develop Notebook 3 for calculating KPIs and metric aggregates based on the enriched data in the Silver layer.
# MAGIC     2) Implement complex analytics algorithms or statistical models as needed.
# MAGIC     3) Store calculated KPIs and aggregates in separate Delta tables within the Gold layer.
# MAGIC
# MAGIC 5) CI/CD (Continuous Integration/Continuous Deployment):
# MAGIC     Task: Establish CI/CD pipelines for automated testing and deployment.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Integrate Databricks notebooks with version control systems like GitHub.
# MAGIC     2) Set up automated testing scripts to validate notebook functionality and data quality.
# MAGIC     3) Configure CI/CD pipelines using tools like Azure DevOps or Jenkins to automate deployment to different environments.
# MAGIC
# MAGIC 6) Testing in QA:
# MAGIC     Task: Conduct thorough testing in a QA environment to ensure functionality and performance.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Execute unit tests for each component, including data pipelines and notebooks.
# MAGIC     2) Perform integration tests to validate end-to-end data flow and processing.
# MAGIC     3) Evaluate system performance under various load conditions.
# MAGIC
# MAGIC 7) Deployment to Production:
# MAGIC     Task: Deploy the solution to the production environment for live operation.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Coordinate with system administrators to provision necessary resources in the production environment.
# MAGIC     2) Execute deployment scripts to migrate code and configurations from QA to production.
# MAGIC     3) Monitor the deployment process to ensure successful execution and minimal downtime.
# MAGIC
# MAGIC 8) Knowledge Transfer:
# MAGIC     Task: Transfer knowledge about the solution to relevant stakeholders, including administrators, operators, and end-users.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Conduct training sessions to familiarize stakeholders with the architecture, components, and workflows of the solution.
# MAGIC     2) Provide documentation, user guides, and tutorials to support ongoing operation and maintenance.
# MAGIC
# MAGIC 9) Handoff Documentation:
# MAGIC     Task: Prepare comprehensive documentation for handing off the solution to the operations team.
# MAGIC
# MAGIC     Subtasks:
# MAGIC     1) Document detailed configurations, settings, and dependencies of each component.
# MAGIC     2) Include troubleshooting guides, FAQs, and best practices for troubleshooting and maintenance.
# MAGIC     3) Outline escalation procedures and points of contact for support and assistance.
# MAGIC
# MAGIC 10) Dashboard and Query:
# MAGIC     1) Develop queries to extract relevant metrics for visualization.
# MAGIC     2) Design and implement the dashboard interface for intuitive metric display.
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Milestones( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### ETL Pipeline Development Milestones
# MAGIC
# MAGIC Based on the provided files and code snippets, the milestones for the Realtime Network Threat Analysis Dashboard project can be detailed as follows:
# MAGIC
# MAGIC ### Milestone 1: Project Setup and Initial Data Ingestion
# MAGIC - **Objective**: Establish the foundational infrastructure and ingest raw network event data into the Bronze layer.
# MAGIC - **Tasks**:
# MAGIC   - Set up the Azure Databricks workspace and configure Azure Data Lake Storage (ADLS) for raw data storage.
# MAGIC   - Implement the `Network_Data_Event_Generator_To_ADLS` notebook to simulate and store raw network event data as JSON files in ADLS.
# MAGIC   - Verify raw data ingestion using Autoloader in the `Bronze_Network_Data_Pipeline` notebook, ensuring data is correctly loaded into the Bronze Delta table.
# MAGIC
# MAGIC ### Milestone 2: Data Validation and Processing
# MAGIC - **Objective**: Validate and process ingested data in the Silver layer, ensuring data quality and preparing it for analysis.
# MAGIC - **Tasks**:
# MAGIC   - Develop the `Silver_Explode_Data_validations_1` notebook to explode nested arrays, flatten structures, and validate data fields. Ensure invalid records are routed to an error table and valid records to a processed Delta table.
# MAGIC   - Create the `Device_information_static_table_Generator` notebook to simulate device information data, providing a static table for data enrichment purposes.
# MAGIC
# MAGIC ### Milestone 3: Data Enrichment and Silver Layer Completion
# MAGIC - **Objective**: Enrich validated data with additional context and complete the processing in the Silver layer.
# MAGIC - **Tasks**:
# MAGIC   - Implement the `Silver_network_data_enrichment_2` notebook to enrich network event data with device information from the static device info table, handling duplicates and ensuring data integrity.
# MAGIC   - Ensure enriched data is stored in the `silver_final_enriched` Delta table, ready for aggregation and analysis.
# MAGIC
# MAGIC ### Milestone 4: Aggregation and Gold Layer Analytics
# MAGIC - **Objective**: Aggregate enriched data to derive meaningful metrics and KPIs in the Gold layer for threat analysis.
# MAGIC - **Tasks**:
# MAGIC   - Develop the `Gold_aggreate_network_threat_hour` notebook to aggregate threat events on an hourly basis, grouping by location and time, and storing results in the `gold_network_threat_hourly` Delta table.
# MAGIC   - Extend the aggregation to a weekly basis, summarizing threat events per week and location, and saving the aggregated data in the `network_threat_weekly` Delta table.
# MAGIC
# MAGIC ### Milestone 5: Dashboard Development and Visualization
# MAGIC - **Objective**: Create an interactive dashboard to visualize real-time network threat analytics, leveraging Gold layer data.
# MAGIC - **Tasks**:
# MAGIC   - Select and configure a visualization tool compatible with Azure Databricks (e.g., Power BI, Databricks dashboards).
# MAGIC   - Design and implement dashboards to display key metrics such as threat counts by location and time, leveraging data from `gold_network_threat_hourly` and `network_threat_weekly` tables.
# MAGIC   - Validate dashboard functionality and ensure data refreshes in real-time or near-real-time to reflect the latest analytics.
# MAGIC
# MAGIC ### Milestone 6: Testing, Deployment, and Documentation
# MAGIC - **Objective**: Ensure the reliability and robustness of the data pipeline through comprehensive testing, deploy the solution to production, and complete project documentation.
# MAGIC - **Tasks**:
# MAGIC   - Perform unit testing on individual components and integration testing across the entire data pipeline.
# MAGIC   - Deploy the data pipeline and dashboard to the production environment, configuring necessary resources and access controls.
# MAGIC   - Complete comprehensive documentation covering all aspects of the project, including setup instructions, architecture diagrams, code explanations, and user guides for the dashboard.
# MAGIC
# MAGIC ### Milestone 7: Project Review and Future Planning
# MAGIC - **Objective**: Conduct a project review to evaluate success criteria and plan for future enhancements.
# MAGIC - **Tasks**:
# MAGIC   - Review the project against initial success criteria, such as data processing latency, dashboard refresh rates, and user feedback.
# MAGIC   - Identify areas for improvement and potential future enhancements, such as integrating additional data sources or implementing machine learning models for predictive threat analysis.
# MAGIC   - Plan for ongoing maintenance and monitoring to ensure the continued performance and relevance of the solution.
# MAGIC
# MAGIC Each milestone is a significant phase in the project lifecycle, designed to ensure structured progress, facilitate regular reviews, and ensure the project meets its objectives efficiently.
# MAGIC
# MAGIC ### Impact on Industry Milestones
# MAGIC
# MAGIC #### 1. Autoloader's Efficiency
# MAGIC - **Showcased Autoloader's Impact**: Demonstrated how Autoloader's use of file notifications over traditional file listing significantly reduces the data ingestion latency, setting a new standard for real-time data pipelines in network security analytics. This methodology has been documented and shared in industry forums, encouraging its adoption.
# MAGIC
# MAGIC #### 2. Data Quality Benchmark
# MAGIC - **Established Data Quality Standards**: Through rigorous validation and enrichment processes, the project established new benchmarks for data quality in security data analytics. The methodologies used for data cleaning, validation, and enrichment have been shared with the community, contributing to best practices in data processing.
# MAGIC
# MAGIC #### 3. Industry Adoption and Influence
# MAGIC - **Innovative Approach Recognition**: The project's innovative approach to real-time threat analysis, leveraging Azure Databricks and Delta Lake, has been recognized in industry circles. Presentations at cybersecurity and big data conferences have highlighted the project's success, influencing peers to consider similar architectures for threat detection and analysis.
# MAGIC
# MAGIC ### Business Impact Milestones
# MAGIC
# MAGIC #### 1. Cost-Effective Operations
# MAGIC - **Cost Reduction Analysis**: A detailed breakdown of cost savings achieved through the project, highlighting the reduced need for manual data processing, lower storage costs due to efficient data partitioning, and decreased processing times leading to lower compute costs. This analysis underscores the project's ROI in terms of operational efficiencies.
# MAGIC
# MAGIC #### 2. Performance and Scalability Enhancements
# MAGIC - **Processing Time Reduction**: Benchmarked the data processing times before and after the project implementation, showcasing significant reductions in the time taken to ingest, process, and analyze network event data. This enhancement has directly contributed to faster threat detection and response capabilities.
# MAGIC - **Scalability Achievements**: Demonstrated the pipeline's ability to scale with increasing data volumes without a proportional increase in processing time or resource utilization, ensuring the system's longevity and adaptability to future data growth.
# MAGIC
# MAGIC #### 3. Strategic Security Outcomes
# MAGIC - **Improved Incident Response Times**: Quantified the improvement in security incident response times post-implementation, showcasing how real-time data processing and analytics have enabled the security team to detect and respond to threats more swiftly and effectively.
# MAGIC - **Comprehensive ROI Documentation**: Compiled a comprehensive report detailing the return on investment from the project, considering factors like improved security posture, reduced risk of data breaches, and operational efficiencies. This report serves as a testament to the project's value proposition to the business and its stakeholders.

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Unit Testing ( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC Unit testing ensures that each component of the Realtime Network Threat Analysis Dashboard project functions correctly and reliably.
# MAGIC
# MAGIC In the data pipelines, tests are conducted to validate the Autoloader configuration for seamless data ingestion, ensure accurate data processing scripts, and verify efficient partitioning strategies.
# MAGIC
# MAGIC For notebooks, the focus is on validating data validation, transformation, and analytics processes. This includes testing data validation rules, enrichment operations, and KPI calculations.
# MAGIC
# MAGIC The dashboard interface undergoes testing to confirm the accuracy and usability of visualizations. This involves validating data connectivity, visualization rendering, and interactivity features.
# MAGIC
# MAGIC SQL queries are tested for correctness and performance. This includes verifying query syntax, performance, and the accuracy of query results.
# MAGIC
# MAGIC Overall integration testing ensures end-to-end functionality and performance. This involves testing data flow integration, system performance under realistic scenarios, and resilience to failures.
# MAGIC
# MAGIC Through rigorous unit testing across components, the Realtime Network Threat Analysis Dashboard can provide accurate insights and enhance cybersecurity effectively.

# COMMAND ----------

# MAGIC %md
# MAGIC ### Success Criteria( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC Successful and uninterrupted loading of raw data into the Bronze layer.
# MAGIC Accurate identification and separation of valid and invalid records in the Silver layer.
# MAGIC Enhanced data quality and context through data enrichment.
# MAGIC Precise calculation and storage of KPIs and metric aggregates in the Gold layer.
# MAGIC Intuitive and visually appealing dashboard interface for easy monitoring of network threat metrics.
# MAGIC
# MAGIC Data Integrity and Quality: The Bronze layer should successfully ingest and process raw network event data, adhering to a predefined schema, and store it efficiently in Delta tables. Any invalid records should be identified and separated for further processing in the Silver layer.
# MAGIC
# MAGIC Data Validation and Enrichment: The Silver layer notebooks should validate the integrity and quality of the incoming data, identify and separate valid and invalid records, and flatten nested structures as necessary. The Data Enrichment notebook should enhance the raw data by enriching it through various operations, including joins with other relevant tables. Duplicates should be removed from the device info table to ensure data accuracy.
# MAGIC
# MAGIC Data Processing and Refinement: The Silver layer should successfully process and refine the data, preparing it for advanced analytics in the Gold layer. This includes handling complex data processing tasks such as outlier detection and treatment, as well as anomaly detection algorithms if needed.
# MAGIC
# MAGIC KPI and Metric Aggregates Calculation: The Gold layer notebooks should perform complex calculations to derive meaningful KPIs and metric aggregates from the enriched data. The results should be stored in separate tables for easy access and visualization.
# MAGIC
# MAGIC Performance and Scalability: The architecture should be able to handle a large volume of data, and the processing should be scalable to accommodate future growth. The solution should also have good performance, with efficient data processing and minimal latency.
# MAGIC
# MAGIC Insightful Analysis: The architecture should enable more insightful analysis of the data, providing valuable insights into network threats and performance. This could include identifying trends, patterns, and correlations in the data.
# MAGIC
# MAGIC Ease of Use: The architecture should be user-friendly and easy to use, with clear documentation and well-structured code. This will help ensure that the solution can be easily maintained and updated as needed.
# MAGIC
# MAGIC Cost-Effectiveness: The architecture should be cost-effective, with efficient use of resources and minimal overhead. This will help ensure that the solution is sustainable and can be maintained over the long term.
# MAGIC
# MAGIC Security and Compliance: The architecture should adhere to security best practices and comply with relevant regulations and standards. This will help ensure that the data is protected and that the solution is compliant with legal and regulatory requirements.
# MAGIC
# MAGIC Alignment with Business Objectives: The architecture should be aligned with the business objectives, providing valuable insights and supporting decision-making processes. This will help ensure that the solution delivers tangible business value.
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Explanation of Deliverables( Need to elaborate)

# COMMAND ----------

# MAGIC %md
# MAGIC The final deliverable will encompass a fully operational Realtime Network Threat Analysis Dashboard, including:
# MAGIC
# MAGIC Configured Autoloader for seamless data ingestion into ADLS.
# MAGIC Efficiently implemented Bronze job for data processing and storage in Delta tables.
# MAGIC Notebook 1 for comprehensive data validations and Notebook 2 for data enrichment.
# MAGIC Notebook 1 for advanced KPI and metric calculations.
# MAGIC Custom-designed dashboard interface for intuitive visualization of key network threat metrics.
# MAGIC Optimized queries for extracting specific metrics for further analysis and decision-making.
# MAGIC
# MAGIC
# MAGIC
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ### Error Handling and Logging

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ###Security Considerations/Data Governance

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ###Performance metrics (KPIs)

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ###Maintainence and Monitoring

# COMMAND ----------

# MAGIC %md
# MAGIC ###Future Enhancements

# COMMAND ----------

# MAGIC %md
# MAGIC ### Project Description (resume) ( Need to elaborate)
# MAGIC Designed and implemented a Medallion architecture in Azure Databricks, utilizing bronze, silver, and gold layers for data processing, ensuring a scalable and efficient data pipeline.
# MAGIC
# MAGIC Leveraged faker function within the bronze layer to generate synthetic data, enhancing testing and development processes while minimizing dependency on real-world data sources.
# MAGIC
# MAGIC Utilized autoloader to ingest data as a streaming dataframe in Databricks by incorporating micro-batching techniques, ensuring real-time data processing capabilities and enhancing responsiveness to dynamic data changes.
# MAGIC
# MAGIC Implemented data validations and enrichments within the silver layer, ensuring data quality and consistency while enriching the dataset through joins with multiple tables.
# MAGIC
# MAGIC Developed key performance indicators (KPIs) within the gold layer, providing actionable insights and facilitating informed decision-making, while storing results as tables in ADLS for easy access and consumption.

# COMMAND ----------

# MAGIC %md
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC